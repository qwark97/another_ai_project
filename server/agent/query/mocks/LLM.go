// Code generated by mockery v2.40.1. DO NOT EDIT.

package mocks

import (
	context "context"

	model "github.com/qwark97/assistant/llms/openai/model"
	mock "github.com/stretchr/testify/mock"
)

// LLM is an autogenerated mock type for the LLM type
type LLM struct {
	mock.Mock
}

type LLM_Expecter struct {
	mock *mock.Mock
}

func (_m *LLM) EXPECT() *LLM_Expecter {
	return &LLM_Expecter{mock: &_m.Mock}
}

// Ask provides a mock function with given fields: ctx, _a1, history
func (_m *LLM) Ask(ctx context.Context, _a1 string, history ...model.Message) (string, error) {
	_va := make([]interface{}, len(history))
	for _i := range history {
		_va[_i] = history[_i]
	}
	var _ca []interface{}
	_ca = append(_ca, ctx, _a1)
	_ca = append(_ca, _va...)
	ret := _m.Called(_ca...)

	if len(ret) == 0 {
		panic("no return value specified for Ask")
	}

	var r0 string
	var r1 error
	if rf, ok := ret.Get(0).(func(context.Context, string, ...model.Message) (string, error)); ok {
		return rf(ctx, _a1, history...)
	}
	if rf, ok := ret.Get(0).(func(context.Context, string, ...model.Message) string); ok {
		r0 = rf(ctx, _a1, history...)
	} else {
		r0 = ret.Get(0).(string)
	}

	if rf, ok := ret.Get(1).(func(context.Context, string, ...model.Message) error); ok {
		r1 = rf(ctx, _a1, history...)
	} else {
		r1 = ret.Error(1)
	}

	return r0, r1
}

// LLM_Ask_Call is a *mock.Call that shadows Run/Return methods with type explicit version for method 'Ask'
type LLM_Ask_Call struct {
	*mock.Call
}

// Ask is a helper method to define mock.On call
//   - ctx context.Context
//   - _a1 string
//   - history ...model.Message
func (_e *LLM_Expecter) Ask(ctx interface{}, _a1 interface{}, history ...interface{}) *LLM_Ask_Call {
	return &LLM_Ask_Call{Call: _e.mock.On("Ask",
		append([]interface{}{ctx, _a1}, history...)...)}
}

func (_c *LLM_Ask_Call) Run(run func(ctx context.Context, _a1 string, history ...model.Message)) *LLM_Ask_Call {
	_c.Call.Run(func(args mock.Arguments) {
		variadicArgs := make([]model.Message, len(args)-2)
		for i, a := range args[2:] {
			if a != nil {
				variadicArgs[i] = a.(model.Message)
			}
		}
		run(args[0].(context.Context), args[1].(string), variadicArgs...)
	})
	return _c
}

func (_c *LLM_Ask_Call) Return(_a0 string, _a1 error) *LLM_Ask_Call {
	_c.Call.Return(_a0, _a1)
	return _c
}

func (_c *LLM_Ask_Call) RunAndReturn(run func(context.Context, string, ...model.Message) (string, error)) *LLM_Ask_Call {
	_c.Call.Return(run)
	return _c
}

// NewLLM creates a new instance of LLM. It also registers a testing interface on the mock and a cleanup function to assert the mocks expectations.
// The first argument is typically a *testing.T value.
func NewLLM(t interface {
	mock.TestingT
	Cleanup(func())
}) *LLM {
	mock := &LLM{}
	mock.Mock.Test(t)

	t.Cleanup(func() { mock.AssertExpectations(t) })

	return mock
}
